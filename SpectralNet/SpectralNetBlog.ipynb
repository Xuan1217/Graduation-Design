{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "AETrainer.py\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load AEtrianer.py successfuully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, architecture: dict, input_dim: int):\n",
    "        super(AE, self).__init__()\n",
    "        self.architecture = architecture\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, self.architecture[\"hidden_dim1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.architecture[\"hidden_dim1\"], self.architecture[\"hidden_dim2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.architecture[\"hidden_dim2\"], self.architecture[\"hidden_dim3\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.architecture[\"hidden_dim3\"], self.architecture[\"output_dim\"]),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.architecture[\"output_dim\"], self.architecture[\"hidden_dim3\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.architecture[\"hidden_dim3\"], self.architecture[\"hidden_dim2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.architecture[\"hidden_dim2\"], self.architecture[\"hidden_dim1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.architecture[\"hidden_dim1\"], input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x) # 编码\n",
    "        x = self.decoder(x) # 解码\n",
    "        return x\n",
    "\n",
    "\n",
    "class AETrainer:\n",
    "    def __init__(self, config: dict, device: torch.device):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            config (dict):          the configuration dictionary\n",
    "            device (torch.device):  the device to train on\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.ae_config = config[\"ae\"]\n",
    "        self.lr = self.ae_config[\"lr\"]\n",
    "        self.epochs = self.ae_config[\"epochs\"]\n",
    "        self.lr_decay = self.ae_config[\"lr_decay\"]\n",
    "        self.patience = self.ae_config[\"patience\"]\n",
    "        self.n_samples = self.ae_config[\"n_samples\"]\n",
    "        self.batch_size = self.ae_config[\"batch_size\"]\n",
    "        self.architecture = self.ae_config[\"architecture\"]\n",
    "        self.weights_path = \"./weights/ae_weights.pth\"\n",
    "\n",
    "    def train(self, X: torch.Tensor) -> AE:\n",
    "        \"\"\"\n",
    "        This function trains the autoencoder on the given data.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor):   the data to train on\n",
    "\n",
    "        Returns:\n",
    "            AE: the trained autoencoder\n",
    "\n",
    "        \"\"\"\n",
    "        self.X = X.view(X.size(0), -1)\n",
    "        self.criterion = nn.MSELoss() # 定义损失使用均方误差\n",
    "        self.ae_net = AE(self.architecture, input_dim=self.X.shape[1]).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.ae_net.parameters(), lr=self.lr)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,  # 学习率优化\n",
    "                                                              mode=\"min\",\n",
    "                                                              factor=self.lr_decay,\n",
    "                                                              patience=self.patience)\n",
    "\n",
    "        if os.path.exists(self.weights_path):\n",
    "            self.ae_net.load_state_dict(torch.load(self.weights_path))\n",
    "            return self.ae_net\n",
    "\n",
    "        train_loader, valid_loader = self._get_data_loader()\n",
    "\n",
    "        print(\"Training Autoencoder:\")\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0.0\n",
    "            for batch_x in train_loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_x = batch_x.view(batch_x.size(0), -1)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.ae_net(batch_x)\n",
    "                loss = self.criterion(output, batch_x)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            valid_loss = self.validate(valid_loader)\n",
    "            self.scheduler.step(valid_loss)\n",
    "            current_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            if current_lr <= self.ae_config[\"min_lr\"]: break\n",
    "            print(\"Epoch: {}/{}, Train Loss: {:.4f}, Valid Loss: {:.4f}, LR: {:.6f}\".\n",
    "            format(epoch + 1, self.epochs, train_loss, valid_loss, current_lr))\n",
    "\n",
    "        torch.save(self.ae_net.state_dict(), self.weights_path)\n",
    "        return self.ae_net\n",
    "\n",
    "    def validate(self, valid_loader: DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        This function validates the autoencoder on the given data during the training process.\n",
    "\n",
    "        Args:\n",
    "            valid_loader (DataLoader):  the data to validate on\n",
    "\n",
    "        Returns:\n",
    "            float: the validation loss\n",
    "        \"\"\"\n",
    "\n",
    "        self.ae_net.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x in valid_loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_x = batch_x.view(batch_x.size(0), -1)\n",
    "                output = self.ae_net(batch_x)\n",
    "                loss = self.criterion(output, batch_x)\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(valid_loader)\n",
    "        return valid_loss\n",
    "\n",
    "    def embed(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This function embeds the given data using the trained autoencoder.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor):  the data to embed\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the embedded data\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Embedding data ...\")\n",
    "        self.ae_net.eval()\n",
    "        with torch.no_grad():\n",
    "            X = X.view(X.size(0), -1)\n",
    "            encoded_data = self.ae_net.encoder(X.to(self.device))\n",
    "        return encoded_data\n",
    "\n",
    "    def _get_data_loader(self) -> tuple:\n",
    "        \"\"\"\n",
    "        This function splits the data into train and validation sets\n",
    "        and returns the corresponding data loaders.\n",
    "\n",
    "        Returns:\n",
    "            tuple: the train and validation data loaders\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.X[:self.n_samples]\n",
    "        trainset_len = int(len(X) * 0.9)\n",
    "        validset_len = len(X) - trainset_len\n",
    "        trainset, validset = random_split(X, [trainset_len, validset_len])\n",
    "        train_loader = DataLoader(trainset, batch_size=self.ae_config[\"batch_size\"], shuffle=True)\n",
    "        valid_loader = DataLoader(validset, batch_size=self.ae_config[\"batch_size\"], shuffle=False)\n",
    "        return train_loader, valid_loader\n",
    "\n",
    "\n",
    "# Finish Point\n",
    "print(\"load AEtrianer.py successfuully!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "data.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data.py successfuully!\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def load_mnist() -> tuple:\n",
    "    tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_set = datasets.MNIST(root='../data',\n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=tensor_transform)\n",
    "    test_set = datasets.MNIST(root='../data',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=tensor_transform)\n",
    "\n",
    "    x_train, y_train = zip(*train_set)\n",
    "    x_train, y_train = torch.cat(x_train), torch.Tensor(y_train)\n",
    "    x_test, y_test = zip(*test_set)\n",
    "    x_test, y_test = torch.cat(x_test), torch.Tensor(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def load_twomoon() -> tuple:\n",
    "        data, y = make_moons(n_samples=7000, shuffle=True, noise=0.075, random_state=None)\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.33, random_state=42)\n",
    "        x_train, x_test = torch.Tensor(x_train), torch.Tensor(x_test)\n",
    "        y_train, y_test = torch.Tensor(y_train), torch.Tensor(y_test)\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "def load_reuters() -> tuple:\n",
    "    with h5py.File('../data/Reuters/reutersidf_total.h5', 'r') as f:\n",
    "        x = np.asarray(f.get('data'), dtype='float32')\n",
    "        y = np.asarray(f.get('labels'), dtype='float32')\n",
    "\n",
    "        n_train = int(0.9 * len(x))\n",
    "        x_train, x_test = x[:n_train], x[n_train:]\n",
    "        y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "    x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
    "    y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def load_smalldata() -> tuple:\n",
    "    mat = loadmat('../data/uci/D153.mat')\n",
    "    x = mat['data']\n",
    "#    x = preprocessing.minmax_scale(x,axis=1)\n",
    "#    x[np.isnan(x)] = 0\n",
    "    y = mat['labels']\n",
    "    y = y.reshape(-1)\n",
    "    n_train = int(0.9 * len(x))\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train], y[n_train:]\n",
    "    x_train, x_test = torch.Tensor(x_train), torch.Tensor(x_test)\n",
    "    y_train, y_test = torch.Tensor(y_train), torch.Tensor(y_test)\n",
    "    # scaler = StandardScaler()\n",
    "    # data = scaler.fit_transform(data)\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.33, random_state=42)\n",
    "    # x_train, x_test = torch.Tensor(x_train), torch.Tensor(x_test)\n",
    "    # y_train, y_test = torch.Tensor(y_train), torch.Tensor(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def load_from_path(dpath: str, lpath: str = None) -> tuple:\n",
    "    X = np.loadtxt(dpath, delimiter=',', dtype=np.float32)\n",
    "    n_train = int(0.9 * len(X))\n",
    "\n",
    "    x_train, x_test = X[:n_train], X[n_train:]\n",
    "    x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
    "\n",
    "    if lpath is not None:\n",
    "        y = np.loadtxt(lpath, delimiter=',', dtype=np.float32)\n",
    "        y_train, y_test = y[:n_train], y[n_train:]\n",
    "        y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "    else:\n",
    "        y_train, y_test = None, None\n",
    "\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def load_data(dataset: str) -> tuple:\n",
    "    \"\"\"\n",
    "    This function loads the dataset specified in the config file.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        dataset (str or dictionary):    In case you want to load your own dataset,\n",
    "                                        you should specify the path to the data (and label if applicable)\n",
    "                                        files in the config file in a dictionary fashion under the key \"dataset\".\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the dataset is not found in the config file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train and test data and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == 'mnist':\n",
    "        x_train, y_train, x_test, y_test = load_mnist()\n",
    "    elif dataset == 'twomoons':\n",
    "        x_train, y_train, x_test, y_test = load_twomoon()\n",
    "    elif dataset == 'reuters':\n",
    "        x_train, y_train, x_test, y_test = load_reuters()\n",
    "    elif dataset == 'smalldata':\n",
    "        x_train, y_train, x_test, y_test = load_smalldata()\n",
    "    else:\n",
    "        try:\n",
    "            data_path = dataset[\"dpath\"]\n",
    "            if \"lpath\" in dataset:\n",
    "                label_path = dataset[\"lpath\"]\n",
    "            else:\n",
    "                label_path = None\n",
    "        except:\n",
    "            raise ValueError(\"Could not find dataset path. Check your config file.\")\n",
    "        x_train, y_train, x_test, y_test = load_from_path(data_path, label_path)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Finish Point\n",
    "print(\"load data.py successfuully!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "metrics.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load metrics.py successfuully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from evaluation import cluster_accuracy as acc1\n",
    "from utils import *\n",
    "from munkres import Munkres\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "from sklearn.metrics import jaccard_score as jaccard\n",
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "from sklearn.metrics import fowlkes_mallows_score as fm\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "class Metrics:\n",
    "    @staticmethod\n",
    "    def acc_score(cluster_assignments: np.ndarray, y: np.ndarray, n_clusters: int)  -> float:\n",
    "        \"\"\"\n",
    "        Computes the accuracy score of the clustering algorithm\n",
    "        Args:\n",
    "            cluster_assignments (np.ndarray):   cluster assignments for each data point\n",
    "            y (np.ndarray):                     ground truth labels\n",
    "            n_clusters (int):                   number of clusters\n",
    "\n",
    "        Returns:\n",
    "            float: accuracy score\n",
    "        \"\"\"\n",
    "\n",
    "        confusion_matrix = metrics.confusion_matrix(y, cluster_assignments, labels=None)\n",
    "        cost_matrix = calculate_cost_matrix(confusion_matrix, n_clusters=n_clusters)\n",
    "        indices = Munkres().compute(cost_matrix)\n",
    "        kmeans_to_true_cluster_labels = get_cluster_labels_from_indices(indices)\n",
    "        y_pred = kmeans_to_true_cluster_labels[cluster_assignments]\n",
    "        print(metrics.confusion_matrix(y, y_pred))\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        return accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def nmi_score(cluster_assignments: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the normalized mutual information score of the clustering algorithm\n",
    "        Args:\n",
    "            cluster_assignments (np.ndarray):   cluster assignments for each data point\n",
    "            y (np.ndarray):                     ground truth labels\n",
    "\n",
    "        Returns:\n",
    "            float: normalized mutual information score\n",
    "        \"\"\"\n",
    "        return nmi(cluster_assignments, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def Jaccard_index(true_labels: np.ndarray, cluster_labels: np.ndarray) -> float:\n",
    "        return jaccard(true_labels, cluster_labels, average='macro')\n",
    "\n",
    "    @staticmethod\n",
    "    def ARI_score(true_labels: np.ndarray, cluster_labels: np.ndarray) -> float:\n",
    "        return ari(true_labels, cluster_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def FM_score(true_labels: np.ndarray, cluster_labels: np.ndarray) -> float:\n",
    "        return fm(true_labels, cluster_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def sil_score(origin_data: np.ndarray, cluster_labels: np.ndarray) -> float:\n",
    "        return silhouette_score(origin_data, cluster_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def c_h_score(origin_data: np.ndarray, cluster_labels: np.ndarray) -> float:\n",
    "        return calinski_harabasz_score(origin_data, cluster_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def d_b_score(origin_data: np.ndarray, cluster_labels: np.ndarray) -> float:\n",
    "        return davies_bouldin_score(origin_data, cluster_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def e_acc(origin_data: np.ndarray, cluster_labels: np.ndarray)  -> float:\n",
    "        return acc1(origin_data, cluster_labels)\n",
    "\n",
    "\n",
    "# Finish Point\n",
    "print(\"load metrics.py successfuully!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SiameseTrainer.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load SiameseTrainer.py successfuully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, architecture: dict, input_dim: int):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.architecture = architecture\n",
    "        self.num_of_layers = self.architecture[\"n_layers\"]\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        current_dim = input_dim\n",
    "        for layer, dim in self.architecture.items():\n",
    "            if layer == \"n_layers\":\n",
    "                continue\n",
    "            next_dim = dim\n",
    "            layer = nn.Sequential(nn.Linear(current_dim, next_dim), nn.ReLU())\n",
    "            self.layers.append(layer)\n",
    "            current_dim = next_dim\n",
    "\n",
    "    def forward_once(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # 24行，前向传播\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> tuple:\n",
    "        output1 = self.forward_once(x1) # 网络1\n",
    "        output2 = self.forward_once(x2) # 网络2，两者共用一个权重\n",
    "        return output1, output2\n",
    "\n",
    "\n",
    "class SiameseDataset:\n",
    "    def __init__(self, pairs: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pairs (list):  A list of tuples containing the\n",
    "                           pairs of data and their labels\n",
    "        \"\"\"\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        x1 = self.pairs[index][0]\n",
    "        x2 = self.pairs[index][1]\n",
    "        label = self.pairs[index][2]\n",
    "        return x1, x2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1: torch.Tensor, output2: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            output1 (torch.Tensor):     First output of the siamese network\n",
    "            output2 (torch.Tensor):     Second output of the siamese network\n",
    "            label (torch.Tensor):       Should be 1 if the two outputs are similar\n",
    "                                        and 0 if they are not\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: loss value\n",
    "        \"\"\"\n",
    "\n",
    "        euclidean = nn.functional.pairwise_distance(output1, output2) # 计算两个输出的距离\n",
    "        positive_distance = torch.pow(euclidean, 2) # 正对：距离平方\n",
    "        negative_distance = torch.pow(torch.clamp(self.margin - euclidean, min=0.0), 2) # 负对\n",
    "        loss = torch.mean((label * positive_distance) + ((1 - label) * negative_distance))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SiameseTrainer:\n",
    "    def __init__(self, config: dict, device: torch.device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (dict):          A dictionary containing the configuration\n",
    "            device (torch.device):  The device to be used for training\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        self.siamese_config = config[\"siamese\"]\n",
    "        self.lr = self.siamese_config[\"lr\"]\n",
    "        self.epochs = self.siamese_config[\"epochs\"]\n",
    "        self.lr_decay = self.siamese_config[\"lr_decay\"]\n",
    "        self.patience = self.siamese_config[\"patience\"]\n",
    "        self.batch_size = self.siamese_config[\"batch_size\"]\n",
    "        self.architecture = self.siamese_config[\"architecture\"]\n",
    "        self.weights_path = \"./weights/siamese_weights.pth\"\n",
    "\n",
    "    def train(self, X: torch.Tensor) -> SiameseNet:\n",
    "        \"\"\"\n",
    "        Trains the siamese network\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor):  The data to be used for training\n",
    "\n",
    "        Returns:\n",
    "            SiameseNet: The trained siamese network\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X.view(X.size(0), -1)\n",
    "        # self.X = X\n",
    "\n",
    "        self.criterion = ContrastiveLoss()\n",
    "        self.siamese_net = SiameseNet(self.architecture, input_dim=self.X.shape[1]).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.siamese_net.parameters(), lr=self.lr)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,\n",
    "                                                              mode=\"min\",\n",
    "                                                              factor=self.lr_decay,\n",
    "                                                              patience=self.patience)\n",
    "\n",
    "        if os.path.exists(self.weights_path):\n",
    "            self.siamese_net.load_state_dict(torch.load(self.weights_path))\n",
    "            return self.siamese_net\n",
    "\n",
    "        train_loader, valid_loader = self._get_data_loader()\n",
    "\n",
    "        print(\"Training Siamese Network:\")\n",
    "        self.siamese_net.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0.0\n",
    "            for x1, x2, label in train_loader:\n",
    "                x1 = x1.to(self.device)\n",
    "                x1 = x1.view(x1.size(0), -1)\n",
    "                x2 = x2.to(self.device)\n",
    "                x2 = x2.view(x2.size(0), -1)\n",
    "                label = label.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output1, output2 = self.siamese_net(x1, x2)\n",
    "                loss = self.criterion(output1, output2, label)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            valid_loss = self.validate(valid_loader)\n",
    "            self.scheduler.step(valid_loss)\n",
    "            current_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            if current_lr <= self.siamese_config[\"min_lr\"]:\n",
    "                break\n",
    "            print(\"Epoch: {}/{}, Train Loss: {:.4f}, Valid Loss: {:.4f}, LR: {:.6f}\".\n",
    "            format(epoch + 1, self.epochs, train_loss, valid_loss, current_lr))\n",
    "\n",
    "        torch.save(self.siamese_net.state_dict(), self.weights_path)\n",
    "        return self.siamese_net\n",
    "\n",
    "    def validate(self, valid_loader: DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        Validates the siamese network\n",
    "\n",
    "        Args:\n",
    "            valid_loader (DataLoader):  The dataloader for the validation data\n",
    "\n",
    "        Returns:\n",
    "            float:  The validation loss\n",
    "        \"\"\"\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        self.siamese_net.eval()\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, label in valid_loader:\n",
    "                x1 = x1.to(self.device)\n",
    "                x1 = x1.view(x1.size(0), -1)\n",
    "                x2 = x2.to(self.device)\n",
    "                x2 = x2.view(x2.size(0), -1)\n",
    "                label = label.to(self.device)\n",
    "                output1, output2 = self.siamese_net(x1, x2)\n",
    "                loss = self.criterion(output1, output2, label)\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(valid_loader)\n",
    "        return valid_loss\n",
    "\n",
    "    def _get_knn_pairs(self) -> list:\n",
    "        \"\"\"\n",
    "        Gets the pairs of data points to be used for training the siamese network.\n",
    "        The pairs are chosen such that each data point has n_neighbors positive pairs\n",
    "        and n_neighbors negative pairs where the neighbors are chosen using KNN\n",
    "\n",
    "        Returns:\n",
    "            list:   A list of pairs of data points\n",
    "        \"\"\"\n",
    "\n",
    "        pairs = []\n",
    "        X = self.X.detach().cpu().numpy()\n",
    "        data_indices = np.arange(len(X))\n",
    "        n_neighbors = self.siamese_config[\"n_neighbors\"]\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors + 1, algorithm=\"ball_tree\").fit(X) # nbrs包括所有点的近邻的索引和距离\n",
    "        _, neighbors_indices = nbrs.kneighbors(X) # 索引\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            non_neighbors_indices = np.delete(data_indices, neighbors_indices[i]) # 非近邻索引\n",
    "            # 随机选择相同数量的非近邻的索引\n",
    "            non_neighbors_random_chosen_indices = np.random.choice(non_neighbors_indices, n_neighbors)\n",
    "\n",
    "            positive_pairs = [[self.X[i], self.X[n], 1] for n in neighbors_indices[i][1:n_neighbors + 1]] # 构造正对，n从索引中获得\n",
    "            negative_pairs = [[self.X[i], self.X[n], 0] for n in non_neighbors_random_chosen_indices]\n",
    "\n",
    "            pairs += positive_pairs\n",
    "            pairs += negative_pairs\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _get_approx_nn_pairs(self) -> list:\n",
    "        \"\"\"\n",
    "        Gets the pairs of data points to be used for training the siamese network.\n",
    "        The pairs are chosen such that each data point has 1 neighbor from its nearest n_neighbors\n",
    "        neighbors and 1 neighbor from the rest of the data points. The neighbors are chosen using\n",
    "        approximate nearest neighbors using Annoy library.\n",
    "\n",
    "        Returns:\n",
    "            list:  A list of pairs of data points\n",
    "        \"\"\"\n",
    "\n",
    "        pairs = []\n",
    "        n_samples = self.siamese_config[\"n_samples\"]\n",
    "        n_neighbors = self.siamese_config[\"n_neighbors\"]\n",
    "        indices = torch.randperm(self.X.shape[0])[:n_samples]\n",
    "        x_train = self.X[indices]\n",
    "        X_numpy = self.X[indices].detach().cpu().numpy()\n",
    "        data_indices = np.arange(len(x_train))\n",
    "\n",
    "        ann = AnnoyIndex(X_numpy.shape[1], 'euclidean')\n",
    "        for i, x_ in enumerate(X_numpy):\n",
    "            ann.add_item(i, x_)\n",
    "        ann.build(50)\n",
    "\n",
    "        neighbors_indices = np.empty((len(X_numpy), n_neighbors + 1))\n",
    "        for i in range(len(X_numpy)):\n",
    "            nn_i = ann.get_nns_by_item(i, n_neighbors + 1, include_distances=False)\n",
    "            neighbors_indices[i, :] = np.array(nn_i)\n",
    "        neighbors_indices = neighbors_indices.astype(int)\n",
    "\n",
    "        print(\"Building dataset for the siamese network ...\")\n",
    "        for i in range(len(X_numpy)):\n",
    "            non_neighbors_indices = np.delete(data_indices, neighbors_indices[i])\n",
    "\n",
    "            neighbor_idx = np.random.choice(neighbors_indices[i][1:], 1)\n",
    "            non_nbr_idx = np.random.choice(non_neighbors_indices, 1)\n",
    "\n",
    "            positive_pairs = [[x_train[i], x_train[neighbor_idx], 1]]\n",
    "            negative_pairs = [[x_train[i], x_train[non_nbr_idx], 0]]\n",
    "\n",
    "            pairs += positive_pairs\n",
    "            pairs += negative_pairs\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _get_pairs(self) -> list:\n",
    "        \"\"\"\n",
    "        Gets the pairs of data points to be used for training the siamese network\n",
    "\n",
    "        Returns:\n",
    "            list: A list of pairs of data points\n",
    "        \"\"\"\n",
    "\n",
    "        should_use_approx = self.siamese_config[\"use_approx\"]\n",
    "        if should_use_approx:\n",
    "            return self._get_approx_nn_pairs()\n",
    "        else:\n",
    "            return self._get_knn_pairs()\n",
    "\n",
    "    def _get_data_loader(self) -> tuple:\n",
    "        \"\"\"\n",
    "        This function splits the data into train and validation sets\n",
    "        and returns the corresponding data loaders.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the train and validation data loaders\n",
    "        \"\"\"\n",
    "\n",
    "        pairs = self._get_pairs()\n",
    "        siamese_dataset = SiameseDataset(pairs)\n",
    "        siamese_trainset_len = int(len(siamese_dataset) * 0.9)\n",
    "        siamese_validset_len = len(siamese_dataset) - siamese_trainset_len\n",
    "        siamese_trainset, siamese_validset = random_split(siamese_dataset, [siamese_trainset_len, siamese_validset_len])\n",
    "        siamese_trainloader = DataLoader(siamese_trainset, batch_size=self.siamese_config[\"batch_size\"], shuffle=True)\n",
    "        siamese_validloader = DataLoader(siamese_validset, batch_size=self.siamese_config[\"batch_size\"], shuffle=False)\n",
    "        return siamese_trainloader, siamese_validloader\n",
    "\n",
    "    # 小批量优化\n",
    "    def get_data_loader_myself(self) -> tuple:\n",
    "        \"\"\"\n",
    "        This function splits the data into train and validation sets\n",
    "        and returns the corresponding data loaders.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the train and validation data loaders\n",
    "        \"\"\"\n",
    "        # 构造正负对的时候就需要修改，构造循环取数据的DataLoader需要给定数目，所以在定义的时候记得记录下真实的数目\n",
    "        # 先分好训练集和验证集，主要是分好数据点(保证验证集中的x不会出现在训练集中即可)\n",
    "        # 所以需要第一个参数：总数据集的大小，不用传参\n",
    "        X = self.X.detach().cpu().numpy()\n",
    "        siamese_trainset_len = int(len(X) * 0.9)\n",
    "        siamese_validset_len = len(X) - siamese_trainset_len\n",
    "        siamese_trainset, siamese_validset = random_split(X, [siamese_trainset_len, siamese_validset_len])\n",
    "        # 开始对训练集进行正负对小批量划分\n",
    "        train_pairs = self.get_pairs_myself(siamese_trainset)\n",
    "        siamese_train_dataset = SiameseDataset(train_pairs)\n",
    "        number = 5 * self.siamese_config[\"batch_size\"]\n",
    "        siamese_trainloader = DataLoader(siamese_train_dataset, batch_size=number, shuffle=False)  # 打乱就没意义了\n",
    "        # 开始对验证集进行正负对小批量划分\n",
    "        valid_pairs = self.get_pairs_myself(siamese_validset)\n",
    "        siamese_valid_dataset = SiameseDataset(valid_pairs)\n",
    "        number = 5 * self.siamese_config[\"batch_size\"]\n",
    "        siamese_validloader = DataLoader(siamese_valid_dataset, batch_size=number, shuffle=False)\n",
    "        return siamese_trainloader, siamese_validloader\n",
    "\n",
    "    def get_pairs_myself(self , X) -> list:\n",
    "        # 先随机取多个小批量，然后分别进行正负对的打包\n",
    "        train_pairs = []\n",
    "        # 取小批量同时记录\n",
    "        m = self.siamese_config[\"batch_size\"]\n",
    "        train_loader = DataLoader(X, batch_size=m, shuffle=True)\n",
    "        for data in train_loader:\n",
    "            # 开始对data进行拆分:0~mid-1求正对，mid~m-1求负对\n",
    "            data_pairs = self.get_knn_data_pairs(data)\n",
    "            train_pairs += data_pairs\n",
    "        return train_pairs\n",
    "\n",
    "    def get_knn_data_pairs(self , X) -> list:\n",
    "        pairs = []\n",
    "        data_indices = np.arange(len(X))\n",
    "        # n_neighbors = self.siamese_config[\"n_neighbors\"]\n",
    "        n_neighbors = 5\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors + 1, algorithm=\"ball_tree\").fit(X) # nbrs包括所有点的近邻的索引和距离\n",
    "        _, neighbors_indices = nbrs.kneighbors(X) # 索引\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            if i < len(X)/2:\n",
    "                positive_pairs = [[self.X[i], self.X[n], 1] for n in neighbors_indices[i][1:n_neighbors + 1]]\n",
    "                pairs += positive_pairs\n",
    "            else:\n",
    "                non_neighbors_indices = np.delete(data_indices, neighbors_indices[i]) # 非近邻索引\n",
    "                non_neighbors_random_chosen_indices = np.random.choice(non_neighbors_indices, n_neighbors)\n",
    "                negative_pairs = [[self.X[i], self.X[n], 0] for n in non_neighbors_random_chosen_indices]\n",
    "                pairs += negative_pairs\n",
    "        return pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Finish Point\n",
    "print(\"load SiameseTrainer.py successfuully!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SpectralTrainer.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load SpectralTrainer.py successfuully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import *\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "class SpectralNetModel(nn.Module):\n",
    "    def __init__(self, architecture: dict, input_dim: int):\n",
    "        super(SpectralNetModel, self).__init__()\n",
    "        self.architecture = architecture\n",
    "        self.num_of_layers = self.architecture[\"n_layers\"]\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        current_dim = self.input_dim\n",
    "        for layer, dim in self.architecture.items():\n",
    "            next_dim = dim\n",
    "            if layer == \"n_layers\":\n",
    "                continue\n",
    "            if layer == \"output_dim\":\n",
    "                layer = nn.Sequential(nn.Linear(current_dim, next_dim), nn.Tanh())\n",
    "                self.layers.append(layer)\n",
    "            else:\n",
    "                layer = nn.Sequential(nn.Linear(current_dim, next_dim), nn.ReLU())\n",
    "                self.layers.append(layer)\n",
    "                current_dim = next_dim\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, is_orthonorm: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This function performs the forward pass of the model.\n",
    "        If is_orthonorm is True, the output of the network is orthonormalized\n",
    "        using the Cholesky decomposition.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor):               The input tensor\n",
    "            is_orthonorm (bool, optional):  Whether to orthonormalize the output or not.\n",
    "                                            Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        Y_tilde = x\n",
    "        if is_orthonorm:\n",
    "            m = Y_tilde.shape[0]\n",
    "            to_factorize = torch.mm(Y_tilde.t(), Y_tilde)\n",
    "            L = torch.linalg.cholesky(to_factorize, upper=False)\n",
    "            L_inverse = torch.inverse(L)\n",
    "            self.orthonorm_weights = np.sqrt(m) * L_inverse.t()\n",
    "\n",
    "        Y = torch.mm(Y_tilde, self.orthonorm_weights)\n",
    "        return Y\n",
    "\n",
    "\n",
    "\n",
    "class SpectralNetLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectralNetLoss, self).__init__()\n",
    "\n",
    "    def forward(self, W: torch.Tensor, Y: torch.Tensor , is_normalized: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This function computes the loss of the SpectralNet model.\n",
    "        The loss is the rayleigh quotient of the Laplacian matrix obtained from W,\n",
    "        and the orthonormalized output of the network.\n",
    "\n",
    "        Args:\n",
    "            W (torch.Tensor):               Affinity matrix\n",
    "            Y (torch.Tensor):               Output of the network\n",
    "            is_normalized (bool, optional): Whether to use the normalized Laplacian matrix or not.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The loss\n",
    "        \"\"\"\n",
    "        m = Y.size(0)\n",
    "        if is_normalized:\n",
    "            D = torch.sum(W, dim=1)\n",
    "            Y = Y / D[:, None]\n",
    "\n",
    "        Dy = torch.cdist(Y, Y)\n",
    "        loss = torch.sum(W * Dy.pow(2)) / (2 * m)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SpectralTrainer:\n",
    "    def __init__(self, config: dict, device: torch.device, is_sparse: bool = False):\n",
    "        \"\"\"\n",
    "        This class is responsible for training the SpectralNet model.\n",
    "\n",
    "        Args:\n",
    "            config (dict):                  The configuration dictionary\n",
    "            device (torch.device):          The device to use for training\n",
    "            is_sparse (bool, optional):     Whether the graph-laplacian obtained from a mini-batch is sparse or not.\n",
    "                                            In case it is sparse, we build the batch by taking 1/5 of the original random batch,\n",
    "                                            and then we add to each sample 4 of its nearest neighbors. Defaults to False.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        self.is_sparse = is_sparse\n",
    "        self.spectral_config = config[\"spectral\"]\n",
    "\n",
    "        self.lr = self.spectral_config[\"lr\"]\n",
    "        self.epochs = self.spectral_config[\"epochs\"]\n",
    "        self.lr_decay = self.spectral_config[\"lr_decay\"]\n",
    "        self.patience = self.spectral_config[\"patience\"]\n",
    "        self.batch_size = self.spectral_config[\"batch_size\"]\n",
    "        self.architecture = self.spectral_config[\"architecture\"]\n",
    "\n",
    "    def train(self, X: torch.Tensor, y: torch.Tensor, siamese_net: nn.Module = None) -> SpectralNetModel:\n",
    "        \"\"\"\n",
    "        This function trains the SpectralNet model.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor):                       The dataset to train on\n",
    "            y (torch.Tensor):                       The labels of the dataset in case there are any\n",
    "            siamese_net (nn.Module, optional):      The siamese network to use for computing the affinity matrix.\n",
    "\n",
    "        Returns:\n",
    "            SpectralNetModel: The trained SpectralNet model\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X.view(X.size(0), -1)\n",
    "        self.y = y\n",
    "        self.counter = 0\n",
    "        self.siamese_net = siamese_net\n",
    "        self.criterion = SpectralNetLoss()\n",
    "        self.spectral_net = SpectralNetModel(self.architecture, input_dim=self.X.shape[1]).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.spectral_net.parameters(), lr=self.lr)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,\n",
    "                                                              mode='min',\n",
    "                                                              factor=self.lr_decay,\n",
    "                                                              patience=self.patience)\n",
    "\n",
    "\n",
    "        train_loader, ortho_loader, valid_loader = self._get_data_loader()\n",
    "\n",
    "        print(\"Training SpectralNet:\")\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0.0\n",
    "            for (X_grad, y_grad), (X_orth, _) in zip(train_loader, ortho_loader):\n",
    "                X_grad = X_grad.to(device=self.device)\n",
    "                X_grad = X_grad.view(X_grad.size(0), -1)\n",
    "                X_orth = X_orth.to(device=self.device)\n",
    "                X_orth = X_orth.view(X_orth.size(0), -1)\n",
    "\n",
    "                if self.is_sparse:\n",
    "                    X_grad = make_batch_for_sparse_grapsh(X_grad)\n",
    "                    X_orth = make_batch_for_sparse_grapsh(X_orth)\n",
    "\n",
    "                # Orthogonalization step\n",
    "                self.spectral_net.eval()\n",
    "                self.spectral_net(X_orth, is_orthonorm=True)\n",
    "\n",
    "                # Gradient step\n",
    "                self.spectral_net.train()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                if self.is_sparse:\n",
    "                    X_grad = make_batch_for_sparse_grapsh(X_grad)\n",
    "\n",
    "                Y = self.spectral_net(X_grad, is_orthonorm=False)\n",
    "                if self.siamese_net is not None:\n",
    "                    with torch.no_grad():\n",
    "                        X_grad = self.siamese_net.forward_once(X_grad)\n",
    "\n",
    "                W = self._get_affinity_matrix(X_grad)\n",
    "\n",
    "                loss = self.criterion(W, Y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            # Validation step\n",
    "            valid_loss = self.validate(valid_loader)\n",
    "            self.scheduler.step(valid_loss)\n",
    "\n",
    "            current_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "            if current_lr <= self.spectral_config[\"min_lr\"]: break\n",
    "            print(\"Epoch: {}/{}, Train Loss: {:.7f}, Valid Loss: {:.7f}, LR: {:.6f}\".\n",
    "            format(epoch + 1, self.epochs, train_loss, valid_loss, current_lr))\n",
    "\n",
    "        return self.spectral_net\n",
    "\n",
    "    def validate(self, valid_loader: DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        This function validates the SpectralNet model during the training process.\n",
    "\n",
    "        Args:\n",
    "            valid_loader (DataLoader):  The validation data loader\n",
    "\n",
    "        Returns:\n",
    "            float: The validation loss\n",
    "        \"\"\"\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        self.spectral_net.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                X, y = batch\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "                if self.is_sparse:\n",
    "                    X = make_batch_for_sparse_grapsh(X)\n",
    "\n",
    "                Y = self.spectral_net(X, is_orthonorm=False)\n",
    "                with torch.no_grad():\n",
    "                    if self.siamese_net is not None:\n",
    "                        X = self.siamese_net.forward_once(X)\n",
    "\n",
    "                W = self._get_affinity_matrix(X)\n",
    "\n",
    "                loss = self.criterion(W, Y)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "        valid_loss /= len(valid_loader)\n",
    "        return valid_loss\n",
    "\n",
    "\n",
    "    def _get_affinity_matrix(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This function computes the affinity matrix W using the Gaussian kernel.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor):   The input data\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The affinity matrix W\n",
    "        \"\"\"\n",
    "\n",
    "        is_local = self.spectral_config[\"is_local_scale\"]  # 局部/全局\n",
    "        n_neighbors = self.spectral_config[\"n_neighbors\"]\n",
    "        scale_k = self.spectral_config[\"scale_k\"]\n",
    "        Dx = torch.cdist(X,X) # 距离\n",
    "        Dis, indices = get_nearest_neighbors(X, k=n_neighbors + 1) # 点的k近邻的距离与索引\n",
    "        scale = compute_scale(Dis, k=scale_k, is_local=is_local) # 计算高斯相似函数的尺度\n",
    "        W = get_gaussian_kernel(Dx, scale, indices, device=self.device, is_local=is_local)\n",
    "        return W\n",
    "\n",
    "\n",
    "    def _get_data_loader(self) -> tuple:\n",
    "        \"\"\"\n",
    "        This function returns the data loaders for training, validation and testing.\n",
    "\n",
    "        Returns:\n",
    "            tuple:  The data loaders\n",
    "        \"\"\"\n",
    "        if self.y is None:\n",
    "            self.y = torch.zeros(len(self.X))\n",
    "        train_size = int(0.9 * len(self.X))\n",
    "        valid_size = len(self.X) - train_size\n",
    "        dataset = TensorDataset(self.X, self.y)\n",
    "        train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        ortho_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader, ortho_loader, valid_loader\n",
    "\n",
    "\n",
    "\n",
    "class ReduceLROnAvgLossPlateau(_LRScheduler):\n",
    "    def __init__(self, optimizer, factor=0.1, patience=10, min_lr=0, verbose=False, min_delta=1e-4):\n",
    "        \"\"\"\n",
    "        Custom ReduceLROnPlateau scheduler that uses the average loss instead of the loss of the last epoch.\n",
    "\n",
    "        Args:\n",
    "            optimizer (_type_):             The optimizer\n",
    "            factor (float, optional):       factor by which the learning rate will be reduced.\n",
    "                                            new_lr = lr * factor. Defaults to 0.1.\n",
    "            patience (int, optional):       number of epochs with no average improvement after\n",
    "                                            which learning rate will be reduced.\n",
    "            min_lr (int, optional):         A lower bound on the learning rate of all param groups.\n",
    "            verbose (bool, optional):       If True, prints a message to stdout for each update.\n",
    "            min_delta (_type_, optional):   threshold for measuring the new optimum, to only focus on\n",
    "                                            significant changes. Defaults to 1e-4.\n",
    "        \"\"\"\n",
    "\n",
    "        self.factor = factor\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.wait = 0\n",
    "        self.best = 1e5\n",
    "        self.avg_losses = []\n",
    "        self.min_lr = min_lr\n",
    "        super(ReduceLROnAvgLossPlateau, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [base_lr * self.factor ** self.num_bad_epochs\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, loss=1.0,  epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "\n",
    "        current_loss = loss\n",
    "        if len(self.avg_losses) < self.patience:\n",
    "            self.avg_losses.append(current_loss)\n",
    "        else:\n",
    "            self.avg_losses.pop(0)\n",
    "            self.avg_losses.append(current_loss)\n",
    "        avg_loss = sum(self.avg_losses) / len(self.avg_losses)\n",
    "        if avg_loss < self.best - self.min_delta:\n",
    "            self.best = avg_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    old_lr = float(param_group['lr'])\n",
    "                    if old_lr > self.min_lr:\n",
    "                        new_lr = old_lr * self.factor\n",
    "                        new_lr = max(new_lr, self.min_lr)\n",
    "                        param_group['lr'] = new_lr\n",
    "                        if self.verbose:\n",
    "                            print(f'Epoch {epoch}: reducing learning rate to {new_lr}.')\n",
    "                self.wait = 0\n",
    "            self.wait += 1\n",
    "\n",
    "\n",
    "# Finish Point\n",
    "print(\"load SpectralTrainer.py successfuully!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "utils.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load utils.py successfuully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "\n",
    "def get_number_of_clusters(X: torch.Tensor,  n_samples: int, threshold: float) -> int:\n",
    "    \"\"\"\n",
    "    Computes the number of clusters in the given dataset\n",
    "\n",
    "    Args:\n",
    "        X:          dataset\n",
    "        n_samples:  number of samples to use for computing the number of clusters\n",
    "        threshold:  threshold for the eigenvalues of the laplacian matrix. This\n",
    "                    threshold is used in order to find when the difference between\n",
    "                    the eigenvalues becomes large.\n",
    "\n",
    "    Returns:\n",
    "        Number of clusters in the dataset\n",
    "    \"\"\"\n",
    "    indices = torch.randperm(X.shape[0])[:n_samples]\n",
    "    X = X[indices]\n",
    "\n",
    "    W = get_affinity_matrix(X)\n",
    "    L = get_laplacian(W)\n",
    "    vals = get_eigenvalues(L)\n",
    "    diffs = np.diff(vals)\n",
    "    cutoff = np.argmax(diffs > threshold)\n",
    "    num_clusters = cutoff + 1\n",
    "    return num_clusters\n",
    "\n",
    "def build_ann(X: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Builds approximate-nearest-neighbors object\n",
    "    that can be used to calculate the knn of a data-point\n",
    "\n",
    "    Args:\n",
    "        X:  dataset\n",
    "    \"\"\"\n",
    "    X = X.view(X.size(0), -1)\n",
    "    t = AnnoyIndex(X[0].shape[0], 'euclidean')\n",
    "    for i, x_i in enumerate(X):\n",
    "        t.add_item(i, x_i)\n",
    "\n",
    "    t.build(50)\n",
    "    t.save('ann_index.ann')\n",
    "\n",
    "\n",
    "def make_batch_for_sparse_grapsh(batch_x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes new batch of data points from the given batch (batch_x)\n",
    "    in case that the graph-laplacian obtained from the given batch is sparse.\n",
    "    The new batch is computed based on the nearest neighbors of 0.25\n",
    "    of the given batch\n",
    "\n",
    "    Args:\n",
    "        batch_x:    Batch of data points\n",
    "\n",
    "    Returns:\n",
    "        New batch of data points\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = batch_x.shape[0]\n",
    "    batch_size //= 5\n",
    "    new_batch_x = batch_x[:batch_size]\n",
    "    batch_x = new_batch_x\n",
    "    n_neighbors = 5\n",
    "\n",
    "    u = AnnoyIndex(batch_x[0].shape[0], 'euclidean')\n",
    "    u.load('ann_index.ann')\n",
    "    for x in batch_x:\n",
    "        x = x.detach().cpu().numpy()\n",
    "        nn_indices = u.get_nns_by_vector(x, n_neighbors)\n",
    "        nn_tensors = [u.get_item_vector(i) for i in nn_indices[1:]]\n",
    "        nn_tensors = torch.tensor(nn_tensors)\n",
    "        new_batch_x = torch.cat((new_batch_x, nn_tensors))\n",
    "\n",
    "    return new_batch_x\n",
    "\n",
    "\n",
    "def get_laplacian(W: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the unnormalized Laplacian matrix, given the affinity matrix W\n",
    "\n",
    "    Args:\n",
    "        W (torch.Tensor):   Affinity matrix\n",
    "\n",
    "    Returns:\n",
    "        Laplacian matrix\n",
    "    \"\"\"\n",
    "\n",
    "    W = W.detach().cpu().numpy()\n",
    "    D = np.diag(W.sum(axis=1))\n",
    "    L = D - W\n",
    "    return L\n",
    "\n",
    "\n",
    "def sort_laplacian(L: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sorts the columns and the rows of the laplacian by the true lablel in order\n",
    "    to see whether the sorted laplacian is a block diagonal matrix\n",
    "\n",
    "    Args:\n",
    "        L:  Laplacian matrix\n",
    "        y:  labels\n",
    "\n",
    "    Returns:\n",
    "        Sorted laplacian\n",
    "    \"\"\"\n",
    "\n",
    "    i = np.argsort(y)\n",
    "    L = L[i, :]\n",
    "    L = L[:, i]\n",
    "    return L\n",
    "\n",
    "\n",
    "def sort_matrix_rows(A: np.ndarray , y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sorts the rows of a matrix by a given order y\n",
    "\n",
    "    Args:\n",
    "        A:  Numpy ndarray\n",
    "        y:  True labels\n",
    "    \"\"\"\n",
    "\n",
    "    i = np.argsort(y)\n",
    "    A = A[i, :]\n",
    "    return A\n",
    "\n",
    "\n",
    "def get_eigenvalues(A: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the eigenvalues of a given matrix A and sorts them in increasing order\n",
    "\n",
    "    Args:\n",
    "        A:  Numpy ndarray\n",
    "\n",
    "    Returns:\n",
    "        Sorted eigenvalues\n",
    "    \"\"\"\n",
    "\n",
    "    _, vals, _ = np.linalg.svd(A)\n",
    "    sorted_vals = vals[np.argsort(vals)]\n",
    "    return sorted_vals\n",
    "\n",
    "\n",
    "def get_eigenvectors(A: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the eigenvectors of a given matrix A and sorts them by the eigenvalues\n",
    "    Args:\n",
    "        A:  Numpy ndarray\n",
    "\n",
    "    Returns:\n",
    "        Sorted eigenvectors\n",
    "    \"\"\"\n",
    "\n",
    "    vecs, vals, _ = np.linalg.svd(A)\n",
    "    vecs = vecs[:, np.argsort(vals)]\n",
    "    return vecs\n",
    "\n",
    "def plot_eigenvalues(vals: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plot the eigenvalues of the laplacian\n",
    "\n",
    "    Args:\n",
    "        vals:   Eigenvalues\n",
    "    \"\"\"\n",
    "\n",
    "    rang = range(len(vals))\n",
    "    plt.plot(rang, vals)\n",
    "    plt.show()\n",
    "\n",
    "def get_laplacian_eigenvectors(V: torch.Tensor, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns eigenvectors of the laplacian when the data is in increasing order by the true label.\n",
    "    i.e., the rows of the eigenvectors matrix V are sorted by the true labels in increasing order.\n",
    "\n",
    "    Args:\n",
    "        V:  Eigenvectors matrix\n",
    "        y:  True labels\n",
    "    \"\"\"\n",
    "\n",
    "    V = sort_matrix_rows(V, y)\n",
    "    rang = range(len(y))\n",
    "    return V, rang\n",
    "\n",
    "def plot_laplacian_eigenvectors(V: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plot the eigenvectors of the laplacian when the data is in increasing order by the true label.\n",
    "    i.e., the rows of the eigenvectors matrix V are sorted by the true labels in increasing order.\n",
    "\n",
    "    Args:\n",
    "        V:  Eigenvectors matrix\n",
    "        y:  True labels\n",
    "    \"\"\"\n",
    "\n",
    "    # sort the rows of V\n",
    "    V = sort_matrix_rows(V, y)\n",
    "    rang = range(len(y))\n",
    "    plt.plot(rang, V)\n",
    "    plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def plot_sorted_laplacian(W: torch.Tensor, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plot the block diagonal matrix that is obtained from the sorted laplacian\n",
    "\n",
    "    Args:\n",
    "        W:  Affinity matrix\n",
    "        y:  True labels\n",
    "    \"\"\"\n",
    "    L = get_laplacian(W)\n",
    "    L = sort_laplacian(L, y)\n",
    "    plt.imshow(L, cmap='hot', norm=colors.LogNorm())\n",
    "    plt.imshow(L, cmap='flag')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_nearest_neighbors(X: torch.Tensor, Y: torch.Tensor = None, k: int = 3) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes the distances and the indices of the\n",
    "    k nearest neighbors of each data point\n",
    "\n",
    "    Args:\n",
    "        X:              Batch of data points\n",
    "        Y (optional):   Defaults to None.\n",
    "        k:              Number of nearest neighbors to calculate. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        Distances and indices of each datapoint\n",
    "    \"\"\"\n",
    "\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    if len(X) < k:\n",
    "        k = len(X)\n",
    "    X = X.cpu().detach().numpy()\n",
    "    Y = Y.cpu().detach().numpy()\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(Y)\n",
    "    Dis, Ids = nbrs.kneighbors(X)\n",
    "    return Dis, Ids\n",
    "\n",
    "\n",
    "def get_grassman_distance(A: np.ndarray, B: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Grassmann distance between the subspaces spanned by the columns of A and B\n",
    "\n",
    "    Args:\n",
    "        A:  Numpy ndarray\n",
    "        B:  Numpy ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    M = np.dot(np.transpose(A), B)\n",
    "    _, s, _ = np.linalg.svd(M, full_matrices=False)\n",
    "    s = 1 - np.square(s)\n",
    "    grassmann = np.sum(s)\n",
    "    return grassmann\n",
    "\n",
    "\n",
    "def compute_scale(Dis: np.ndarray, k: int = 2, med: bool = True, is_local: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the scale for the Gaussian similarity function\n",
    "\n",
    "    Args:\n",
    "        Dis:        Distances of the k nearest neighbors of each data point.\n",
    "        k:          Number of nearest neighbors. Defaults to 2.\n",
    "        med:        Scale calculation method. Can be calculated by the median distance\n",
    "                    from a data point to its neighbors, or by the maximum distance.\n",
    "        is_local:   Local distance (different for each data point), or global distance. Defaults to local.\n",
    "\n",
    "    Returns:\n",
    "        scale (global or local)\n",
    "    \"\"\"\n",
    "\n",
    "    if is_local:\n",
    "        if not med:\n",
    "            scale = np.max(Dis, axis=1) # 按列求最大值\n",
    "        else:\n",
    "            scale = np.median(Dis, axis=1) # 按列取中位数\n",
    "    else:\n",
    "        if not med:\n",
    "            scale = np.max(Dis[:, k - 1]) # 第k-1列取最大\n",
    "        else:\n",
    "            scale = np.median(Dis[:, k - 1])\n",
    "    return scale\n",
    "\n",
    "\n",
    "def get_gaussian_kernel(D: torch.Tensor, scale, Ids: np.ndarray, device: torch.device, is_local: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the Gaussian similarity function\n",
    "    according to a given distance matrix D and a given scale\n",
    "\n",
    "    Args:\n",
    "        D:      Distance matrix\n",
    "        scale:  scale\n",
    "        Ids:    Indices of the k nearest neighbors of each sample\n",
    "        device: Defaults to torch.device(\"cpu\")\n",
    "        is_local:  Determines whether the given scale is global or local\n",
    "\n",
    "    Returns:\n",
    "        Matrix W with Gaussian similarities\n",
    "    \"\"\"\n",
    "\n",
    "    if not is_local:\n",
    "        # global scale\n",
    "        W = torch.exp(-torch.pow(D, 2) / (scale ** 2))\n",
    "    else:\n",
    "        # local scales\n",
    "        W = torch.exp(-torch.pow(D, 2).to(device) / (torch.tensor(scale).float().to(device).clamp_min(1e-7) ** 2))\n",
    "    if Ids is not None:\n",
    "        n, k = Ids.shape\n",
    "        mask = torch.zeros([n, n]).to(device=device)\n",
    "        for i in range(len(Ids)):\n",
    "            mask[i, Ids[i]] = 1\n",
    "        W = W * mask\n",
    "    sym_W = (W + torch.t(W)) / 2. # 对称\n",
    "    return sym_W\n",
    "\n",
    "\n",
    "def plot_data_by_assignmets(X, assignments: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plots the data with the assignments obtained from SpectralNet.\n",
    "    Relevant only for 2D data\n",
    "\n",
    "    Args:\n",
    "        X:                      Data\n",
    "        cluster_assignments:    Cluster assignments\n",
    "    \"\"\"\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=assignments)\n",
    "    plt.show()\n",
    "\n",
    "def calculate_cost_matrix(C: np.ndarray , n_clusters: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the cost matrix for the Munkres algorithm\n",
    "\n",
    "    Args:\n",
    "        C (np.ndarray):     Confusion matrix\n",
    "        n_clusters (int):   Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:        Cost matrix\n",
    "    \"\"\"\n",
    "    cost_matrix = np.zeros((n_clusters, n_clusters))\n",
    "    # cost_matrix[i,j] will be the cost of assigning cluster i to label j\n",
    "    for j in range(n_clusters):\n",
    "        s = np.sum(C[:, j])  # number of examples in cluster i\n",
    "        for i in range(n_clusters):\n",
    "            t = C[i, j]\n",
    "            cost_matrix[j, i] = s - t\n",
    "    return cost_matrix\n",
    "\n",
    "\n",
    "def get_cluster_labels_from_indices(indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gets the cluster labels from their indices\n",
    "\n",
    "    Args:\n",
    "        indices (np.ndarray):  Indices of the clusters\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:   Cluster labels\n",
    "    \"\"\"\n",
    "\n",
    "    num_clusters = len(indices)\n",
    "    cluster_labels = np.zeros(num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        cluster_labels[i] = indices[i][1]\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def write_assignmets_to_file(assignments: np.ndarray):\n",
    "    \"\"\"\n",
    "    Saves SpectralNet cluster assignments to a file\n",
    "\n",
    "    Args:\n",
    "        assignments (np.ndarray): The assignments that obtained from SpectralNet\n",
    "    \"\"\"\n",
    "\n",
    "    np.savetxt(\"cluster_assignments.csv\", assignments.astype(int), fmt='%i', delimiter=',')\n",
    "\n",
    "\n",
    "def create_weights_dir():\n",
    "    \"\"\"\n",
    "    Creates a directory for the weights of the Autoencoder and the Siamese network\n",
    "    \"\"\"\n",
    "    if not os.path.exists('weights'):\n",
    "        os.makedirs('weights')\n",
    "\n",
    "\n",
    "def get_affinity_matrix(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the affinity matrix W\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor):  Data\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Affinity matrix W\n",
    "    \"\"\"\n",
    "    is_local = True\n",
    "    n_neighbors = 30\n",
    "    scale_k = 15\n",
    "    Dx = torch.cdist(X,X)\n",
    "    Dis, indices = get_nearest_neighbors(X, k=n_neighbors + 1)\n",
    "    scale = compute_scale(Dis, k=scale_k, is_local=is_local)\n",
    "    W = get_gaussian_kernel(Dx, scale, indices, device=torch.device(\"cpu\"), is_local=is_local)\n",
    "    return W\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Finish Point\n",
    "print(\"load utils.py successfuully!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SpectralNet.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load SpectralNet.py successfuully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from AETrainer import *\n",
    "from SiameseTrainer import *\n",
    "from SpectralTrainer import *\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "class SpectralNet:\n",
    "    def __init__(self, n_clusters: int, config: dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_clusters (int):   The dimension of the projection subspace\n",
    "            config (dict):      The configuration dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.config = config\n",
    "        self.embeddings_ = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    def fit(self, X: torch.Tensor ,y: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Performs the main training loop for the SpectralNet model.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor):   Data to train the networks on\n",
    "            y (torch.Tensor):   Labels in case there are any. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        should_use_ae = self.config[\"should_use_ae\"]\n",
    "        should_use_siamese = self.config[\"should_use_siamese\"]\n",
    "        create_weights_dir()\n",
    "\n",
    "        if should_use_ae:\n",
    "            ae_trainer = AETrainer(self.config, self.device)\n",
    "            self.ae_net = ae_trainer.train(X)\n",
    "            X = ae_trainer.embed(X)\n",
    "\n",
    "        if should_use_siamese:\n",
    "            siamese_trainer = SiameseTrainer(self.config, self.device)\n",
    "            self.siamese_net = siamese_trainer.train(X)\n",
    "        else:\n",
    "            self.siamese_net = None\n",
    "\n",
    "        is_sparse = self.config[\"is_sparse_graph\"]\n",
    "        if is_sparse:\n",
    "            build_ann(X) # util.py；构建可用于计算数据点的knn的近似近邻对象\n",
    "        spectral_trainer = SpectralTrainer(self.config, self.device, is_sparse=is_sparse)\n",
    "        self.spec_net = spectral_trainer.train(X, y, self.siamese_net)\n",
    "\n",
    "\n",
    "    def predict(self, X: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts the cluster assignments for the given data.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor):   Data to be clustered\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:  The cluster assignments for the given data\n",
    "\n",
    "        \"\"\"\n",
    "        X = X.view(X.size(0), -1)\n",
    "        X = X.to(self.device)\n",
    "        should_use_ae = self.config[\"should_use_ae\"]\n",
    "        if should_use_ae:\n",
    "            X = self.ae_net.encoder(X)\n",
    "        self.embeddings_ = self.spec_net(X, is_orthonorm = False).detach().cpu().numpy()\n",
    "        cluster_assignments = self._get_clusters_by_kmeans(self.embeddings_)\n",
    "        return cluster_assignments\n",
    "\n",
    "\n",
    "    def _get_clusters_by_kmeans(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Performs k-means clustering on the spectral-embedding space.\n",
    "\n",
    "        Args:\n",
    "            embeddings (np.ndarray):   the spectral-embedding space\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:  the cluster assignments for the given data\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters).fit(embeddings)\n",
    "        cluster_assignments = kmeans.predict(embeddings)\n",
    "        return cluster_assignments\n",
    "\n",
    "\n",
    "\n",
    "# Finish Point\n",
    "print(\"load SpectralNet.py successfuully!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "main.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder:\n",
      "Epoch: 1/30, Train Loss: 0.0545, Valid Loss: 0.0400, LR: 0.001000\n",
      "Epoch: 2/30, Train Loss: 0.0311, Valid Loss: 0.0273, LR: 0.001000\n",
      "Epoch: 3/30, Train Loss: 0.0242, Valid Loss: 0.0227, LR: 0.001000\n",
      "Epoch: 4/30, Train Loss: 0.0208, Valid Loss: 0.0204, LR: 0.001000\n",
      "Epoch: 5/30, Train Loss: 0.0189, Valid Loss: 0.0193, LR: 0.001000\n",
      "Epoch: 6/30, Train Loss: 0.0178, Valid Loss: 0.0194, LR: 0.001000\n",
      "Epoch: 7/30, Train Loss: 0.0169, Valid Loss: 0.0174, LR: 0.001000\n",
      "Epoch: 8/30, Train Loss: 0.0163, Valid Loss: 0.0171, LR: 0.001000\n",
      "Epoch: 9/30, Train Loss: 0.0158, Valid Loss: 0.0166, LR: 0.001000\n",
      "Epoch: 10/30, Train Loss: 0.0153, Valid Loss: 0.0161, LR: 0.001000\n",
      "Epoch: 11/30, Train Loss: 0.0149, Valid Loss: 0.0158, LR: 0.001000\n",
      "Epoch: 12/30, Train Loss: 0.0146, Valid Loss: 0.0155, LR: 0.001000\n",
      "Epoch: 13/30, Train Loss: 0.0143, Valid Loss: 0.0153, LR: 0.001000\n",
      "Epoch: 14/30, Train Loss: 0.0140, Valid Loss: 0.0151, LR: 0.001000\n",
      "Epoch: 15/30, Train Loss: 0.0138, Valid Loss: 0.0150, LR: 0.001000\n",
      "Epoch: 16/30, Train Loss: 0.0136, Valid Loss: 0.0145, LR: 0.001000\n",
      "Epoch: 17/30, Train Loss: 0.0134, Valid Loss: 0.0146, LR: 0.001000\n",
      "Epoch: 18/30, Train Loss: 0.0132, Valid Loss: 0.0143, LR: 0.001000\n",
      "Epoch: 19/30, Train Loss: 0.0131, Valid Loss: 0.0143, LR: 0.001000\n",
      "Epoch: 20/30, Train Loss: 0.0129, Valid Loss: 0.0141, LR: 0.001000\n",
      "Epoch: 21/30, Train Loss: 0.0127, Valid Loss: 0.0139, LR: 0.001000\n",
      "Epoch: 22/30, Train Loss: 0.0126, Valid Loss: 0.0140, LR: 0.001000\n",
      "Epoch: 23/30, Train Loss: 0.0125, Valid Loss: 0.0138, LR: 0.001000\n",
      "Epoch: 24/30, Train Loss: 0.0124, Valid Loss: 0.0138, LR: 0.001000\n",
      "Epoch: 25/30, Train Loss: 0.0123, Valid Loss: 0.0134, LR: 0.001000\n",
      "Epoch: 26/30, Train Loss: 0.0122, Valid Loss: 0.0135, LR: 0.001000\n",
      "Epoch: 27/30, Train Loss: 0.0121, Valid Loss: 0.0137, LR: 0.001000\n",
      "Epoch: 28/30, Train Loss: 0.0120, Valid Loss: 0.0135, LR: 0.001000\n",
      "Epoch: 29/30, Train Loss: 0.0119, Valid Loss: 0.0134, LR: 0.001000\n",
      "Epoch: 30/30, Train Loss: 0.0118, Valid Loss: 0.0131, LR: 0.001000\n",
      "Embedding data ...\n",
      "Training Siamese Network:\n",
      "Epoch: 1/100, Train Loss: 0.0262, Valid Loss: 0.0236, LR: 0.001000\n",
      "Epoch: 2/100, Train Loss: 0.0229, Valid Loss: 0.0227, LR: 0.001000\n",
      "Epoch: 3/100, Train Loss: 0.0219, Valid Loss: 0.0221, LR: 0.001000\n",
      "Epoch: 4/100, Train Loss: 0.0214, Valid Loss: 0.0219, LR: 0.001000\n",
      "Epoch: 5/100, Train Loss: 0.0210, Valid Loss: 0.0221, LR: 0.001000\n",
      "Epoch: 6/100, Train Loss: 0.0207, Valid Loss: 0.0219, LR: 0.001000\n",
      "Epoch: 7/100, Train Loss: 0.0205, Valid Loss: 0.0214, LR: 0.001000\n",
      "Epoch: 8/100, Train Loss: 0.0203, Valid Loss: 0.0216, LR: 0.001000\n",
      "Epoch: 9/100, Train Loss: 0.0201, Valid Loss: 0.0212, LR: 0.001000\n",
      "Epoch: 10/100, Train Loss: 0.0199, Valid Loss: 0.0211, LR: 0.001000\n",
      "Epoch: 11/100, Train Loss: 0.0198, Valid Loss: 0.0214, LR: 0.001000\n",
      "Epoch: 12/100, Train Loss: 0.0196, Valid Loss: 0.0212, LR: 0.001000\n",
      "Epoch: 13/100, Train Loss: 0.0195, Valid Loss: 0.0213, LR: 0.001000\n",
      "Epoch: 14/100, Train Loss: 0.0194, Valid Loss: 0.0211, LR: 0.001000\n",
      "Epoch: 15/100, Train Loss: 0.0192, Valid Loss: 0.0212, LR: 0.001000\n",
      "Epoch: 16/100, Train Loss: 0.0191, Valid Loss: 0.0211, LR: 0.001000\n",
      "Epoch: 17/100, Train Loss: 0.0190, Valid Loss: 0.0212, LR: 0.001000\n",
      "Epoch: 18/100, Train Loss: 0.0190, Valid Loss: 0.0214, LR: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 109\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    108\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m--> 109\u001B[0m     embeddings, assignments \u001B[38;5;241m=\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m    111\u001B[0m     cost_time \u001B[38;5;241m=\u001B[39m end_time \u001B[38;5;241m-\u001B[39m start_time\n",
      "Cell \u001B[1;32mIn[50], line 53\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m     spectralnet \u001B[38;5;241m=\u001B[39m SpectralNet(n_clusters\u001B[38;5;241m=\u001B[39mn_clusters, config\u001B[38;5;241m=\u001B[39mconfig)\n\u001B[1;32m---> 53\u001B[0m     \u001B[43mspectralnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_LinAlgError:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidMatrixException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe output of the network is not a valid matrix to the orthogonalization layer. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     57\u001B[0m                                  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTry to decrease the learning rate to fix the problem.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mG:\\spectral_pytorch\\SpectralNet-main\\src\\SpectralNet.py:44\u001B[0m, in \u001B[0;36mSpectralNet.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_siamese:\n\u001B[0;32m     43\u001B[0m     siamese_trainer \u001B[38;5;241m=\u001B[39m SiameseTrainer(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m---> 44\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msiamese_net \u001B[38;5;241m=\u001B[39m \u001B[43msiamese_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msiamese_net \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mG:\\spectral_pytorch\\SpectralNet-main\\src\\SiameseTrainer.py:142\u001B[0m, in \u001B[0;36mSiameseTrainer.train\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    140\u001B[0m output1, output2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msiamese_net(x1, x2)\n\u001B[0;32m    141\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(output1, output2, label)\n\u001B[1;32m--> 142\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    144\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mE:\\Anaconda\\envs\\spectralNet_py\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Anaconda\\envs\\spectralNet_py\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from data import load_data\n",
    "from metrics import Metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from SpectralNet import SpectralNet\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class InvalidMatrixException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    config_path = \"config/mnist.json\"\n",
    "    with open (config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    dataset = config[\"dataset\"]\n",
    "    n_clusters = config[\"n_clusters\"]\n",
    "    should_check_generalization = config[\"should_check_generalization\"]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = load_data(dataset)\n",
    "\n",
    "    if not should_check_generalization:\n",
    "        if y_train is None:\n",
    "            x_train = torch.cat([x_train, x_test])\n",
    "\n",
    "        else:\n",
    "            x_train = torch.cat([x_train, x_test])\n",
    "            y_train = torch.cat([y_train, y_test])\n",
    "\n",
    "    try:\n",
    "        spectralnet = SpectralNet(n_clusters=n_clusters, config=config)\n",
    "        spectralnet.fit(x_train, y_train)\n",
    "\n",
    "    except torch._C._LinAlgError:\n",
    "        raise InvalidMatrixException(\"The output of the network is not a valid matrix to the orthogonalization layer. \"\n",
    "                                     \"Try to decrease the learning rate to fix the problem.\")\n",
    "\n",
    "    if not should_check_generalization:\n",
    "\n",
    "        cluster_assignments = spectralnet.predict(x_train) # return：标签，SpectralNet.py\n",
    "        if y_train is not None:\n",
    "            y = y_train.detach().cpu().numpy()\n",
    "            acc_score = Metrics.acc_score(cluster_assignments, y, n_clusters) # metrics.py\n",
    "            nmi_score = Metrics.nmi_score(cluster_assignments, y)\n",
    "            J_score = Metrics.Jaccard_index(y, cluster_assignments)\n",
    "            ari_score = Metrics.ARI_score(y, cluster_assignments)\n",
    "            fm_score = Metrics.FM_score(y, cluster_assignments)\n",
    "            e_acc = Metrics.e_acc(y, cluster_assignments)\n",
    "            embeddings = spectralnet.embeddings_\n",
    "            #silhouette_avg = Metrics.sil_score(x_train, cluster_assignments)\n",
    "            #calinski_harabasz_score_val = Metrics.c_h_score(x_train, cluster_assignments)\n",
    "            #davies_bouldin_score_val = Metrics.d_b_score(x_train, cluster_assignments)\n",
    "\n",
    "            print(f\"ACC: {np.round(acc_score, 3)}\")\n",
    "            print(f\"E_ACC: {np.round(e_acc, 3)}\")\n",
    "            print(f\"NMI: {np.round(nmi_score, 3)}\")\n",
    "            print(f\"Jaccard: {np.round(J_score, 3)}\")\n",
    "            print(f\"ARI: {np.round(ari_score, 3)}\")\n",
    "            print(f\"FM: {np.round(fm_score, 3)}\")\n",
    "            # print(f\"silhouette: {np.round(silhouette_avg, 3)}\")\n",
    "            # print(f\"calinski: {np.round(calinski_harabasz_score_val, 3)}\")\n",
    "            # print(f\"davies_bouldin: {np.round(davies_bouldin_score_val, 3)}\")\n",
    "\n",
    "            return embeddings, cluster_assignments\n",
    "\n",
    "    else:\n",
    "        y_test = y_test.detach().cpu().numpy()\n",
    "        spectralnet.predict(x_train)\n",
    "        train_embeddings = spectralnet.embeddings_\n",
    "        test_assignments = spectralnet.predict(x_test)\n",
    "        test_embeddings = spectralnet.embeddings_\n",
    "        kmeans_train = KMeans(n_clusters=n_clusters).fit(train_embeddings) # 训练集使用神经网络嵌入后kmeans聚类\n",
    "        dist_matrix = cdist(test_embeddings, kmeans_train.cluster_centers_) # 计算与质心的距离\n",
    "        closest_cluster = np.argmin(dist_matrix, axis=1) # 取最小，分到相应的簇\n",
    "        acc_score = Metrics.acc_score(closest_cluster, y_test, n_clusters)\n",
    "        e_acc = Metrics.e_acc(y_test, closest_cluster)\n",
    "        nmi_score = Metrics.nmi_score(closest_cluster, y_test)\n",
    "        print(f\"ACC: {np.round(acc_score, 3)}\")\n",
    "        print(f\"NMI: {np.round(nmi_score, 3)}\")\n",
    "        print(f\"E_ACC: {np.round(e_acc, 3)}\")\n",
    "\n",
    "        return test_embeddings, test_assignments\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    embeddings, assignments = main()\n",
    "    end_time = time.time()\n",
    "    cost_time = end_time - start_time\n",
    "    # tsne = TSNE(n_components=2, init='random', random_state=501)\n",
    "    # X_tsne = tsne.fit_transform(embeddings)\n",
    "    #\n",
    "    # plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=assignments, cmap='viridis')\n",
    "    # #plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', label='Cluster Centers')\n",
    "    # plt.xlabel('t-SNE Dimension 1')\n",
    "    # plt.ylabel('t-SNE Dimension 2')\n",
    "    # plt.title('SpectralNet(both) Clustering with t-SNE Visualization')\n",
    "    # plt.legend()\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "    # write_assignmets_to_file(assignments)\n",
    "    print(f\"The time cost: {np.round(cost_time, 3)} s\")\n",
    "    print(\"Your assignments were saved to the file 'cluster_assignments.csv!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, init='random', random_state=501)\n",
    "X_tsne = tsne.fit_transform(embeddings)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=assignments, cmap='viridis')\n",
    "#plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', label='Cluster Centers')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2 ')\n",
    "plt.title(\"Dimensionally reduced data of MNIST dataset\")\n",
    "plt.legend()\n",
    "plt.colorbar()\n",
    "plt.savefig(\"../figures/mnist.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
